---
title: "SPSS to R Conversion Course - Part 1"
author: "Rhys Davies"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    toc_float: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(DescTools)
library(rstatix)
library(jtools)
library(correlation)
library(ggstats)
library(palmerpenguins)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this. To run the code, click the small green arrow in the code chunk, or press `ctrl` + `enter` together.

For anyone who want access to a useful resource to continue working through R, I recommend the [Pirates's](https://bookdown.org/ndphillips/YaRrr/t-test-t-test.html) guide to **RRRRRR**.

## Session aims

-   Learn how to download and run R studio.
-   Learn how navigate the basics of R.
-   Learn data manipulation in R.
-   Learn how to conduct statistical analyses.
-   Learn how to visualise your analyses.

## Getting started

Today, we will be working with the [**Palmer Penguins**](https://allisonhorst.github.io/palmerpenguins/) data set. This data set investigates the differences in flipper length, and bill length + depth, between different species of Penguins, across different island, and across the sex of Penguins.

Please note that there are multiple ways to conduct our analyses in R. So don't be afraid if you've come across something different in the past. Just use the method you find that works best for you.

## Viewing our data

```{r penguins}
summary(penguins) # summary stats of our data
str(penguins) # inspecting data structure
head(penguins) # view first 6 rows 
view(penguins) # Look at the whole data set. Not advised for big data projects.
```

## Exploring data through Plots

We can also use R to generate plots, for example:

```{r }
#base R
plot(penguins$species, 
     penguins$flipper_length_mm ) 

plot(penguins$bill_length_mm,
     penguins$flipper_length_mm ) 

hist(penguins$flipper_length_mm)
boxplot(penguins$flipper_length_mm )
```

```{r}
# ggplot - grammar of graphics
## Here we have assinged our plot code to "plot_1", using the assingment arrow `<-`.
plot_1 <- ggplot(data = penguins, 
       aes(x = species, y = flipper_length_mm)) +
  geom_boxplot()

plot_1

## We can use the assigned object to further customise our code if we want to
plot_1 + facet_wrap(~sex)  + 
  labs(title = "Pretty Penguins Plot",
       subtitle = "My sister was bitten by a penguin once...",
       x = "Penguin Species",
       y = "Flipper Length"
       )


## We can also use ggplot to visualise statistical models
plot_2 <- ggplot(data = penguins, 
       aes(x = bill_length_mm, y = flipper_length_mm, color = species)) +
  geom_point() + 
  geom_smooth(method = "lm") 
  

plot_2 

plot_2 +  facet_grid(~sex ~island) +
  labs(title = "Another Pretty Penguins Plot",
       subtitle = str_wrap("No realli! She was karving her initials on the penguin with the sharpened end of an interspace toothbrush given to her by Svenge...", 80),
       x = "Bill length (mm)",
       y = "Flipper Length (mm)") + 
  theme(legend.position = "bottom")
```

## Conducting analyses

Now that we've seen some examples of data visualisation, and seen some basics of working with R, it's time to run some analyses on the data. On the menu today, we will run through: - t-tests - ANOVAS (and posthoc tests) - Correlation analyses - Regression analyses

## T - test

Our classic t-test. Our go-to tool for investigating differences between 2 groups. For this example, we are going to investigate if there is a significant difference in Flipper Length between male and female penguins. But in order to do so, we will first need to examine our data to see if it is appropriate

```{r}
summary(penguins$sex)

```

As we can see, there are some NA's in the dataset. So, unfortunately, we will need to filter out the androgynous penguins in order to run the analysis (Sorry Penguin Bowie). This will be done with the `filter()` function. And for simplicity, we will remove all other NA's from the data.

```{r}

t_test_data <- penguins %>% 
  filter(
sex != "NA's", # != is the logical operator for "does not include" - this will remove the NA's from sex.
species == "Gentoo"
  ) %>% 
  na.omit() # na.omit removes NA from our data.

summary(t_test_data$sex)
```

Now that our data is appropriately filtered for the t-test, it's time to run some quick diagnostics to determine if our assumptions are met.

### Assumption tests

```{r}
# Checking normality
hist(t_test_data$flipper_length_mm, breaks = 30) 
shapiro.test(t_test_data$flipper_length_mm)
ks.test(t_test_data$flipper_length_mm, 'pnorm')

# Checking for equal variances - this is to determine which t test can be used
LeveneTest(flipper_length_mm ~ sex, data = t_test_data)

## As normality is not equal, the double peak in histogram may be caused by sex differences (as indicated by unequal variance). We will use ggplot to check.
```

```{r}
ggplot(t_test_data,
       aes(x = bill_length_mm, fill = sex)) +
  geom_histogram(position = "dodge") +
  facet_wrap(~sex) +
  labs(title = "Normal distribution, within each sex",
       subtitle = "We apologise for the fault in the subtitles...")

## re-running tests - normality assumed!
norm_test_m <- t_test_data %>% 
  filter(sex == "male") 
  shapiro.test(norm_test_m$flipper_length_mm)
  
  norm_test_f <- t_test_data %>% 
  filter(sex == "female") 
  shapiro.test(norm_test_m$flipper_length_mm)
```

### The analysis

So our data meets the normality assumption, but does not meet the equal variance assumption. What do we do? Well, we opt for the Welch's t-test. Thankfully, this is already the default option for R. Additionally, many [applied statistical researchers](https://research.tue.nl/en/publications/why-psychologists-should-by-default-use-welchs-t-test-instead-of-) are calling for its use over the student t-test, as it is more robust to unequal variances, and performs just as well with equal variance. That is, it does both with less risk of misleading outputs.

```{r}

## Default Welch's 
t_test(flipper_length_mm ~ sex, data = t_test_data) # rstaxix version

t.test(flipper_length_mm ~ sex, data = t_test_data) #base R version

# Student (if needed)

t_test(flipper_length_mm ~ sex, var.equal = TRUE, data = t_test_data) # rstaxix version

t.test(flipper_length_mm ~ sex, var.equal = TRUE,data = t_test_data) #base R version
```

```{r}
### Summary statistics for reporting

t_test_data %>% 
  group_by(sex) %>% 
  summarise(mean_flipper_length = round(mean(flipper_length_mm), 2),
            sd_flipper_length =round(sd(flipper_length_mm), 2))
```

### Plotting our results

```{r}
ggplot(t_test_data,
       aes(x = sex, y = flipper_length_mm))+
  geom_boxplot() +
  theme_apa() +
  labs(title = "Sex Differences in Gentoo Penguin Flipper Lengths",
       subtitle = "Mind you, Penguin bites kan be pretty nastiii...",
       x = "Penguin sex",
       y = "Flipper Length (mm)")

```

## ANOVA (way to analyse our data in R)

Now we have covered our T-test, let's have a dive into the ANOVA world. This time we will examine differences in flipper length between the different species of penguin.

```{r}

anova_data <- penguins %>% 
  na.omit()

```

### Assumption checking

As previously, we need to test our assumptions for the ANOVA, else we risk misleading results. And once again, we

```{r}

LeveneTest(flipper_length_mm ~ species, data = anova_data)

## As normality is not equal, the double peak in histogram may be caused by sex differences (as indicated by unequal variance). We will use ggplot to check.

ggplot(anova_data,
       aes(x = flipper_length_mm, fill = species)) +
  geom_histogram(position = "dodge") +
  facet_wrap(~species) +
  labs(title = "Normal distribution, within each sex",
       subtitle = "We apologise again for the fault in the subtitles...")
```

```{r}
## re-running tests - normality assumed!
norm_test_a <- anova_data %>% 
  filter(species == "Adelie") 
  shapiro.test(norm_test_a$flipper_length_mm)
  
  norm_test_c <- anova_data %>% 
  filter(species == "Chinstrap") 
  shapiro.test(norm_test_c$flipper_length_mm)
  
  norm_test_g <- anova_data %>% 
  filter(species == "Gentoo") 
  shapiro.test(norm_test_g$flipper_length_mm)
```

So our data is normally distributed across the 3 species (apart from Gentoo). But it approximates normality, and so we can commit our analyses.

### The analysis

Now it's time to define our analysis. In R, we define our ANOVA model using the `aov` function. From there, our go to formula is: `aov(outcome_variable ~ predictor_group_variable, data)`.

We then use summary to view the results, and can run the model through post-hoc tests if required.

```{r}

# Defining our model
model <- aov(flipper_length_mm ~ species, data = anova_data)

# Viewing results
summary(model)


# Conducting post hoc analyses

PostHocTest(model, 
            method = "bonferroni", 
            conf.level = 0.95, ordered = FALSE)
```

### Summary statistics.

To get hold of our summary statistics, we need to do some data wrangling. This will make use of the `tidyverse` family of functions.

First off, we use `group_by()` to tell R which categorical variable we want our summaries to be focused on. We can use as many group variables as we like here, but make sure your order follows any hierarchical patterns of your data... Otherwise, the interpretation might be odd.

From there, we use `summarise()` to provide our summarised statistics across our chosen group variable. We choose a meaningful name, and then apply a function to our variable. As seen below, you can combine functions. We're wrapping ours in the `round()` function, so that we can decide how many decimal places we want the analysis to go to.

```{r summary_stats}



anova_data %>% 
  group_by(species) %>%  # Try playing around with the "sex" and "island" variables here. 
  summarise(mean_flipper_length = round(mean(flipper_length_mm), 2),
            sd_flipper_length =round(sd(flipper_length_mm), 2))

# task: include a `median()` summarised statistic for flipper_length. Call it median_flipper_length (if you like).

```

### Visualising results

Now that we have conducted our ANOVA, it's time to visualise the analysis to ensure that our understanding of the analysis is accurate.

Here we will overlap different plot aspects to communicate our model assumptions. The `geom_violin()` is used to show the distributions of each group. The `geom_boxplot()` to communicate the **median** and **inter quartile range** of our data groups.

There are so many options for customising, so it can be adjusted entierly for your needs.

```{r}


ggplot(anova_data,
       aes(x = species,
           y = flipper_length_mm,
           fill = species) # fill allows us to fill an object colour by a variable. Useful here for making a more visually appealing plot.
       )+
  geom_violin( # violin plots helps us visualise distributions
             alpha = .7 # alpha adjusts transparency of points
             ) +
  geom_boxplot(alpha = .7, width = .5) + 
  theme_apa() +
  theme(legend.position = "none") + # 
  labs(title = "Penguininal Differences in Flipper Lengths",
       subtitle = "...",
       x = "Penguin Species",
       y = "Flipper Length (mm)")


```

## Correlation analyses

Now to move on to examining correlations. Once again, we will need to perform some initial data tidying to ensure we can run our analyses.

```{r}

corr_data <- penguins %>%
  select(flipper_length_mm, bill_length_mm, bill_depth_mm) %>%
  na.omit() # we're using na.omit to remove missing values. Another useful function!

stratified_corr_data <- penguins %>%
  select(flipper_length_mm, bill_length_mm,
         bill_depth_mm, species) %>%
  group_by(species) %>% 
  na.omit()


```

### Assumption tests - note

For correlations, we need to inspect the residuals (the distance each point on a scatter graph has between itself and the line of best fit). So we will temporarily delay the assumption tests for now, and jump straight to our analysis.

### Analysis

```{r}


results <- correlation(corr_data)
results
summary(results, redundant = TRUE)



strat_results <- correlation(stratified_corr_data)
strat_results
summary(strat_results, redundant = TRUE)

```

### Back to assumption checking

For correlations, testing the assumptions is best done through visualising the data. Now before we even get to residuals, play around and compare the plots in when species is and is not accounted for. Behold the [Simpson Paradox](https://en.wikipedia.org/wiki/Simpson%27s_paradox) in its glory!

```{r}
# data visualisation

ggplot(penguins,
       aes(x =bill_length_mm, y = flipper_length_mm
          # ,color = species
           )) +
  geom_point(position = "jitter", alpha = .7 ) +
  geom_smooth(method = "lm")

ggplot(penguins,
       aes(x =bill_depth_mm, y = flipper_length_mm
           #,color = species
           )) +
  geom_point(position = "jitter", alpha = .7 ) +
  geom_smooth(method = "lm")

ggplot(penguins,
       aes(x =bill_length_mm, y = bill_depth_mm
          # ,color = species
           )) +
  geom_point(position = "jitter", alpha = .7 ) +
  geom_smooth(method = "lm")

```

## Regression analysis

And now to the final stages of this session. Let's regress to the regression. We will focus here on predicting penguin flipper length. We will also use some features of R to help you statistically determine which model best explains the variance in the penguin data.

```{r}

regression_data <- penguins %>% 
  na.omit()

```

### Analysis

Defining our linear models in R is nice and simple. It follows a similar formula to the `aov()` model setting from above. We set our outcome variable first, and then define our predictor variables. As a bonus, R will also automatically dummy code any predictor variables for us!

For those interested in interaction/moderation analyses - R also makes this very simple. Replace the `+` with a `*` and it will add the interaction term to your model. Play around with the models below if you want to test it.

```{r}
# Step 1 - defining our models
## general pattern: lm(outcome ~ predictor, data)
model_1 <- lm(flipper_length_mm ~ bill_length_mm,
              data = regression_data )


model_2 <- lm(flipper_length_mm ~ bill_length_mm + bill_depth_mm ,
              data = regression_data )

model_3 <- lm(flipper_length_mm ~ bill_length_mm + bill_depth_mm + sex ,
              data = regression_data )

```

```{r}
# Step 2 - viewing and interpreting results
summary(model_1)
summary(model_2)
summary(model_3)
```

### Standardised Coefficients (Beta)

Unfortunatley, R does not provide standardised coefficents by default, and getting hold of them ourselves takes a little more work. But fortunatley, converting the output to get hold of standardised coefficients has been provided for you here! It makes use of the `scale()` function, which standardises your *continuous* variables by turning them into Z-scores. This allows for clearer comparison of effect sizes between coefficients.

As categorical variables are already dummy coded, they do not need to be standardised - as they already have been to some extent.

```{r}
# Step 1 - defining our models
## general pattern: lm(outcome ~ predictor, data)
standardised_model_3 <- lm(scale(flipper_length_mm) ~ scale(bill_length_mm) + scale(bill_depth_mm) + sex,
              data = regression_data )


## Compare standardised and default model. What similarities and differences do you see? 
summary(standardised_model_3)
summary(model_3)

```

### Comparing analyses

We can use the `anova()` function to statistically compare our models on the basis of sum of squares and degrees of freedom. R will then perfom an F test to help us determine if one model significantly improves upon the other.

```{r}
# Step 3 - statistically comparing models
anova(model_1, model_2, model_3)
anova(model_2, model_3)
```

### Assumption tests

By wrapping our model in the `plot()` function, we can visualise the assumption tests underlying our analyses.

```{r}
# Step 4 - assumption tests
plot(model_1)

```

### Visualising our data

Lets say we want to visualise our third model. We have a few different coefficents to take care of. Thankfully, ggplot allows for to play with a variety of

```{r}

summary(model_3)

ggplot(data = regression_data, # definining the data
       aes(x= bill_length_mm, # main predictor variable
            col = bill_depth_mm, # setting color as 2nd predictor
           y = flipper_length_mm) # y is our outcome variable
       ) +
  geom_point(position = "jitter", # jitter adds a teeny amount of movement to each point. Useful for overlapping data
             alpha = .7  # alpha is a measure of transparency. Useful for overlapping points
             ) +
  geom_smooth(method = "lm")+
  facet_wrap(~sex) + # Finally setting sex as another predictor by faceting
  theme_apa()+
  theme(legend.position = "right") +
  labs(title = "We can use ggplot to visualise multiple dimensions",
       subtitle = "This can be very useful for regression models",
       caption = "Just be careful not to overcomplicate it",
       x = "Bill Length (mm)",
       y = "Flipper Length (mm)",
       color = "Bill Depth (mm)"
       )

```

### Coefficent plotting - bonus extra

We can also R to do some fancy coefficient plotting - with confidence intervals. This can be a useful way to help improve our understanding of our model through using a visual representation of our coefficents. This is achieved using the `ggstats` package, which allows for quick plots, and versatility with the `ggplot` features.

```{r}
# Coefficent plot
ggcoef_table(model_1)
ggcoef_table(model_2)
ggcoef_table(model_3)
```

The `ggstats` package also allows us to directly compare our models through the `ggcoef_compare()` function. This can be very useful in detecting and understanding any mediations and potential moderation effects that might be present in our analysis.

### Comparing coefficent plots

```{r}
# Comparing coefficent plots

## step 1 - create a list item of our models
models <- list(
  "basic model" = model_1,
  "second model"  = model_2,
  "full model" = model_3
)

ggcoef_compare(models) # All in one plot
ggcoef_compare(models, type = "faceted") # Faceted to split by model - easier to interpret.

```

## Using regression for categorical data?! (bonus section)

Linear regressions, ANOVA's, and t-test are practically the same thing (mathematically, the ANOVA and the t-test are just specific forms of linear models).

R is more forgiving than SPSS here, as we do not need to manually dummy code our categorical variables! Less accidental errors for us.

This allows for us to play around and compare between these analyses to help improve our conceptual understanding of the analyses we're conducting.

```{r}

reg_model <- lm(flipper_length_mm ~ species ,
                regression_data)
summary(reg_model)

```

We can transform our linear model into an ANOVA as well. Setting + 0 removes the intercept, and replaces it with the reference variable. This in effect converts our analysis to an ANOVA with post-hoc tests.

Try comparing the output of the linear regression model with the `anova_style_model` below. What do you notice about the coefficents?

```{r}
# 

anova_style_model <- lm(flipper_length_mm ~ species  + 0,
                        regression_data)

summary(anova_style_model)
```

### ANOVAS and post-hoc example

Now let's re-run our ANOVA and post-hoc tests to compare with the regression outputs. What do we notice?

```{r}
 
anova_model <- aov(flipper_length_mm ~ species , regression_data)

summary(anova_model)

post_hoc <- PostHocTest(anova_model, 
            method = "lsd", 
            conf.level = 0.95, ordered = FALSE)

post_hoc
```

### Comparing linear models

Comparing outputs - look at the F value, model significance, df's and even the model coefficents.

What similarities/differences do you notice?

```{r}

 
summary(reg_model)
summary(anova_model)
summary(anova_style_model)
post_hoc
```

### Using data visualisation

Now have a look at the box plot chart from below. How does this translate to each of the model summaries above?

```{r}


ggplot(regression_data, 
       aes( x= species, y = flipper_length_mm, fill = species)) +
  geom_point(position = "jitter", alpha = .7) +
  geom_boxplot(alpha = .6)

```
